{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0eaf70f-42ce-4d2c-b86e-f20bde964ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e2311-e29e-4756-8f13-41fa23461b49",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4371a832-f005-4adc-b94a-bc865c0d68b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simonp/.local/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/simonp/.local/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/simonp/.local/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/simonp/.local/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import rcParams, cycler\n",
    "\n",
    "CUDA_VISIBLE_DEVICES = \"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import sklearn\n",
    "from torch import nn\n",
    "import os\n",
    "from src.utils.pool_metrics import get_PCC, get_qc_euclidean, get_batches_euclidean, get_euclidean\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "\n",
    "from src.utils.batch_effect_removal import get_berm  # , remove_batch_effect\n",
    "from src.dl.models.pytorch.aedann import Encoder2, Decoder2, Classifier, Classifier2, Classifier3, DispAct, MeanAct\n",
    "from src.utils.utils import scale_data, get_unique_labels # , scale_data_per_batch\n",
    "from src.dl.models.pytorch.utils.utils import to_categorical\n",
    "from src.dl.models.pytorch.utils.plotting import confidence_ellipse\n",
    "from src.dl.models.pytorch.utils.stochastic import GaussianSample\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tabulate import tabulate\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "# from itertools import cycle\n",
    "# from sklearn.cross_decomposition import CCA\n",
    "# from src.utils.pool_metrics import log_pool_metrics\n",
    "# from src.dl.models.pytorch.aedann import ReverseLayerF\n",
    "#from src.dl.models.pytorch.utils.loggings import TensorboardLoggingAE, log_input_ordination, \\\n",
    "#    log_confusion_matrix, log_plots\n",
    "# from src.dl.models.pytorch.utils.dataset import get_loaders\n",
    "# from datetime import datetime\n",
    "# from src.dl.models.pytorch.utils.utils import log_traces, get_best_loss_from_tb, get_best_acc_from_tb, get_best_values, add_to_logger, get_empty_dicts, get_empty_traces, get_optimizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "strategy = 'CU_DEM-AD'\n",
    "# csv_name = 'unique_genes.csv'\n",
    "# bad_batches = ''\n",
    "# remove_zeros = 0\n",
    "log_stuff = False\n",
    "best_correction = False\n",
    "n_meta = 2\n",
    "train_models = False\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(args=[])\n",
    "args.csv_file = 'unique_genes.csv'\n",
    "args.remove_zeros = 0\n",
    "args.bad_batches = ''\n",
    "args.log1p = 1\n",
    "args.zinb = 0\n",
    "args.groupkfold = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9dc0cf-b6fe-404d-9afe-7525f86fa53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_space = [\n",
    "    # Integer(100, 20000, 'uniform', name='features_cutoff'),\n",
    "    # Real(0, 1, 'uniform', name='threshold'),\n",
    "    Integer(1, 100, 'uniform', name=\"max_features\"),\n",
    "    Integer(2, 10, 'uniform', name=\"min_samples_split\"),\n",
    "    Integer(1, 10, 'uniform', name=\"min_samples_leaf\"),\n",
    "    Integer(1, 1000, 'uniform', name=\"n_estimators\"),\n",
    "    Categorical(['gini', 'entropy'], name=\"criterion\"),\n",
    "    Categorical([True, False], name=\"oob_score\"),\n",
    "    Categorical(['balanced'], name=\"class_weight\"),\n",
    "]\n",
    "linsvc_space = [\n",
    "    # Integer(1, 20000, 'uniform', name='features_cutoff'),\n",
    "    # Real(0, 1, 'uniform', name='threshold'),\n",
    "    Real(1e-4, 1, 'log-uniform', name='tol'),\n",
    "    Integer(1, 1000, 'uniform', name='max_iter'),\n",
    "    Categorical(['l2'], name='penalty'),\n",
    "    Real(1e-3, 10000, 'uniform', name='C'),\n",
    "    Categorical(['balanced'], name='class_weight'),\n",
    "\n",
    "]\n",
    "logreg_space = [\n",
    "    # Integer(1, 20000, 'uniform', name='features_cutoff'),\n",
    "    # Real(0, 1, 'uniform', name='threshold'),\n",
    "    Integer(1, 20000, 'uniform', name='max_iter'),\n",
    "    Real(1e-3, 20000, 'uniform', name='C'),\n",
    "    Categorical(['saga'], name='solver'),\n",
    "    Categorical(['l1', 'l2'], name='penalty'),\n",
    "    Categorical([True, False], name='fit_intercept'),\n",
    "    Categorical(['balanced'], name='class_weight'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a41dc7c-668f-4eed-8c58-7a0456b24e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "def pyGPCA(data, group, name, metrics):\n",
    "    gPCA = importr('gPCA')\n",
    "\n",
    "    newdata = robjects.r.matrix(robjects.FloatVector(np.array(data['inputs'][group]).reshape(-1)), nrow=data['inputs'][group].shape[0])\n",
    "    new_batches = robjects.r.matrix(robjects.IntVector(data['batches'][group]), nrow=data['inputs'][group].shape[0])\n",
    "\n",
    "    results = gPCA.gPCA_batchdetect(newdata, new_batches)\n",
    "    # print(results)\n",
    "    # with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    #     results = np.array(robjects.conversion.rpy2py(results))\n",
    "    # print(results)\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        results = { key : np.array(robjects.conversion.rpy2py(results.rx2(key))) for key in results.names }\n",
    "        # metrics['raw']['delta'] = metrics['raw']['delta']))\n",
    "\n",
    "    if 'pool' in name:\n",
    "        delta_name = 'delta_pool'\n",
    "        name = name.split('_')[1]\n",
    "    else:\n",
    "        delta_name = 'delta'\n",
    "    if name not in metrics:\n",
    "        metrics[name] = {}\n",
    "        # print(results)\n",
    "    metrics[name][delta_name] = results['delta'][0]\n",
    "    # metrics[name]['delta.pval'] = results[1]\n",
    "    return metrics, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc25231d-4875-45d7-a3db-358da693fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "standard_minmax_scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "robust_minmax_scaler = Pipeline([('robust', RobustScaler()), ('minmax', MinMaxScaler())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d4af45b-3079-4f23-8df5-baf76cd273a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dl.models.pytorch.utils.loggings import log_LDA, log_ORD\n",
    "from src.utils.data_getters import get_mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2843549d-0dd4-4a83-9d69-ec406847df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.pool_metrics import log_pool_metrics\n",
    "from src.dl.models.pytorch.utils.loggings import get_metrics, log_metrics, batch_f1_score\n",
    "# from src import log_metrics\n",
    "# log_metrics(logger, lists, values, model, unique_labels, unique_batches, epoch, mlops, metrics, n_meta_emb=0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2765abf2-8ec2-4dc1-b411-304b96069e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def use_pycombat(berm, data):\n",
    "    df = pd.concat((\n",
    "        data['inputs']['train'].copy(),  data['inputs']['valid'].copy(), data['inputs']['test'].copy(), \n",
    "        data['inputs']['train_pool'].copy(),  data['inputs']['valid_pool'].copy(), data['inputs']['test_pool'].copy(), \n",
    "    ))\n",
    "    all_batches = np.concatenate((\n",
    "        data['batches']['train'], data['batches']['valid'], data['batches']['test'],\n",
    "        data['batches']['train_pool'], data['batches']['valid_pool'], data['batches']['test_pool'],\n",
    "        \n",
    "    ))\n",
    "    # assert np.sum(all_batches != np.concatenate((data['batches']['train'], data['batches']['valid'], data['batches']['test']))) == 0\n",
    "    tmp = berm(df.T, all_batches).T\n",
    "    tmp = np.nan_to_num(tmp, 0)\n",
    "    previous_len = 0\n",
    "    for g in ['train', 'valid', 'test', 'train_pool', 'valid_pool', 'test_pool']:\n",
    "        # print(g, previous_len, data['inputs'][g].shape[0])\n",
    "        data['inputs'][g] = pd.DataFrame(\n",
    "            tmp[previous_len:previous_len + data['inputs'][g].shape[0]],\n",
    "            index=data['inputs'][g].index)\n",
    "        previous_len += data['inputs'][g].shape[0]\n",
    "        # print(g, previous_len, data['inputs'][g].shape[0])\n",
    "    data['inputs']['all'] = pd.concat((\n",
    "        data['inputs']['train'], data['inputs']['valid'], data['inputs']['test'],\n",
    "        # data[key]['train_pool'], data[key]['valid_pool'], data[key]['test_pool'],\n",
    "    ), 0)\n",
    "    data['inputs']['all_pool'] = pd.concat((\n",
    "        data['inputs']['train_pool'], data['inputs']['valid_pool'], data['inputs']['test_pool'],\n",
    "    ), 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_batch_effect(berm, data):\n",
    "    \"\"\"\n",
    "    All dataframes have a shape of N samples (rows) x M features (columns)\n",
    "\n",
    "    Args:\n",
    "        berm: Batch effect removal method\n",
    "        all_data: Pandas dataframe containing all data (train, valid and test data)\n",
    "        data: list of Pandas dataframe containing the training data. Used only to get\n",
    "        valid_data: Pandas dataframe containing the validation data\n",
    "        test_data: Pandas dataframe containing the test data\n",
    "        all_batches: A list containing the batch ids corresponding to all_data\n",
    "\n",
    "    Returns:\n",
    "        Returns:\n",
    "        A dictionary of pandas datasets corrected for batch effect with keys:\n",
    "            'all': Pandas dataframe containing all data (train, valid and test data)\n",
    "            'train': Pandas dataframe containing the training data\n",
    "            'valid': Pandas dataframe containing the validation data\n",
    "            'test: Pandas dataframe containing the test data\n",
    "\n",
    "    \"\"\"\n",
    "    if berm is not None:  # Look if has 20 before and after\n",
    "        df = pd.concat((\n",
    "            data['inputs']['train'].copy(),  data['inputs']['valid'].copy(), data['inputs']['test'].copy(), \n",
    "            data['inputs']['train_pool'].copy(),  data['inputs']['valid_pool'].copy(), data['inputs']['test_pool'].copy(), \n",
    "        ))\n",
    "        all_batches = np.concatenate((\n",
    "            data['batches']['train'], data['batches']['valid'], data['batches']['test'],\n",
    "            data['batches']['train_pool'], data['batches']['valid_pool'], data['batches']['test_pool'],\n",
    "\n",
    "        ))\n",
    "        # assert np.sum(all_batches != np.concatenate((data['batches']['train'], data['batches']['valid'], data['batches']['test']))) == 0\n",
    "        tmp = berm(df, all_batches)\n",
    "        inputs_len = data['inputs']['train'].shape[0] + data['inputs']['train'].shape[0] + data['inputs']['valid'].shape[0] + \\\n",
    "                        data['inputs']['valid'].shape[0] + data['inputs']['test'].shape[0] + data['inputs']['test'].shape[0]\n",
    "        previous_len = 0\n",
    "        for g in ['train', 'valid', 'test', 'train_pool', 'valid_pool', 'test_pool']:\n",
    "            if 'all' in g:\n",
    "                continue\n",
    "            print(g, previous_len)\n",
    "            data['inputs'][g] = pd.DataFrame(\n",
    "                tmp[previous_len:previous_len + data['inputs'][g].shape[0]],\n",
    "                index=data['inputs'][g].index)\n",
    "            previous_len += data['inputs'][g].shape[0]\n",
    "        data['inputs']['all'] = pd.concat((\n",
    "            data['inputs']['train'], data['inputs']['valid'], data['inputs']['test'],\n",
    "            # data[key]['train_pool'], data[key]['valid_pool'], data[key]['test_pool'],\n",
    "        ), 0)\n",
    "        data['inputs']['all_pool'] = pd.concat((\n",
    "            data['inputs']['train_pool'], data['inputs']['valid_pool'], data['inputs']['test_pool'],\n",
    "        ), 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_batch_effect_with_classes(berm, data, classes):\n",
    "    \"\"\"\n",
    "    All dataframes have a shape of N samples (rows) x M features (columns)\n",
    "\n",
    "    Args:\n",
    "        berm: Batch effect removal method\n",
    "        all_data: Pandas dataframe containing all data (train, valid and test data)\n",
    "        data: list of Pandas dataframe containing the training data. Used only to get\n",
    "        valid_data: Pandas dataframe containing the validation data\n",
    "        test_data: Pandas dataframe containing the test data\n",
    "        all_batches: A list containing the batch ids corresponding to all_data\n",
    "\n",
    "    Returns:\n",
    "        Returns:\n",
    "        A dictionary of pandas datasets corrected for batch effect with keys:\n",
    "            'all': Pandas dataframe containing all data (train, valid and test data)\n",
    "            'train': Pandas dataframe containing the training data\n",
    "            'valid': Pandas dataframe containing the validation data\n",
    "            'test: Pandas dataframe containing the test data\n",
    "\n",
    "    \"\"\"\n",
    "    if berm is not None:  # Look if has 20 before and after\n",
    "        df = pd.concat((\n",
    "            data['inputs']['train'].copy(), data['inputs']['valid'].copy(), data['inputs']['test'].copy()\n",
    "        ))\n",
    "        all_batches = np.concatenate((\n",
    "            data['batches']['train'], data['batches']['valid'], data['batches']['test']\n",
    "        ))\n",
    "        # assert np.sum(all_batches != np.concatenate((data['batches']['train'], data['batches']['valid'], data['batches']['test']))) == 0\n",
    "        tmp = berm(df, all_batches, classes)\n",
    "        inputs_len = data['inputs']['train'].shape[0] + data['inputs']['valid'].shape[0] + data['inputs']['test'].shape[0]\n",
    "        previous_len = 0\n",
    "        for g in list(data['inputs'].keys()):\n",
    "            if 'all' in g:\n",
    "                continue\n",
    "            print(g, previous_len)\n",
    "            data['inputs'][g] = pd.DataFrame(\n",
    "                tmp[previous_len:previous_len + data['inputs'][g].shape[0]],\n",
    "                index=data['inputs'][g].index)\n",
    "            previous_len += data['inputs'][g].shape[0]\n",
    "        try:\n",
    "            data['inputs']['all'] = pd.DataFrame(tmp, index=df.index, columns=df.columns)\n",
    "        except:\n",
    "            data['inputs']['all'] = pd.DataFrame(tmp, index=df.index)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0500fd53-7fdd-449a-86ce-254b3f818747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(fig, name, acc, mcc, group):\n",
    "    # sns_plot = sns.heatmap(df, annot=True, square=True, cmap=\"YlGnBu\",\n",
    "    #                        annot_kws={\"size\": 35 / np.sqrt(len(df))})\n",
    "    # fig = sns_plot.get_figure()\n",
    "    dirs = '/'.join(name.split('/')[:-1])\n",
    "    name = name.split('/')[-1]\n",
    "    plt.title(f'Confusion Matrix (acc={np.round(np.mean(acc), 2)} +- {np.round(np.std(acc), 2)}, mcc={np.round(np.mean(mcc), 2)} +- {np.round(np.std(mcc), 2)})')\n",
    "    os.makedirs(f'{dirs}/', exist_ok=True)\n",
    "    stuck = True\n",
    "    while stuck:\n",
    "        try:\n",
    "            fig.savefig(f\"{dirs}/cm_{name}_{group}.png\")\n",
    "            stuck = False\n",
    "        except:\n",
    "            print('stuck...')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb0e68db-9d46-4db7-9997-b787830bfe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from src.ml.train.sklearn_train_nocv import count_labels, get_confusion_matrix, save_roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import matthews_corrcoef as MCC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from src.utils.utils import plot_confusion_matrix\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, name, model, data, hparams_names, log_path, args, logger, ovr, model_name='RF', binary=True, mlops='None'):\n",
    "        self.best_roc_score = -1\n",
    "        self.ovr = ovr\n",
    "        self.binary = binary\n",
    "        self.args = args\n",
    "        self.log_path = log_path\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.data = data\n",
    "        self.logger = logger\n",
    "        self.hparams_names = hparams_names\n",
    "        # self.train_indices, self.test_indices, _ = split_train_test(self.labels)\n",
    "        # self.n_splits = args.n_splits\n",
    "        # self.n_repeats = args.n_repeats\n",
    "        # self.jackknife = args.jackknife\n",
    "        self.best_scores_train = -1\n",
    "        self.best_scores_valid = -1\n",
    "        self.best_mccs_train = -1\n",
    "        self.best_mccs_valid = -1\n",
    "        self.scores_train = None\n",
    "        self.scores_valid = None\n",
    "        self.mccs_train = None\n",
    "        self.mccs_valid = None\n",
    "        self.y_preds = np.array([])\n",
    "        self.y_valids = np.array([])\n",
    "        self.iter = 0\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.mlops = mlops\n",
    "        self.best_params_dict = {}\n",
    "        self.best_params_dict_values = {}\n",
    "\n",
    "    def train(self, h_params, groupkfold=1):\n",
    "        self.iter += 1\n",
    "        features_cutoff = None\n",
    "        param_grid = {}\n",
    "        for name, param in zip(self.hparams_names, h_params):\n",
    "            if name == 'features_cutoff':\n",
    "                features_cutoff = param\n",
    "            elif name == 'threshold':\n",
    "                threshold = param\n",
    "            else:\n",
    "                param_grid[name] = param\n",
    "        scores_valid = []\n",
    "        scores_train = []\n",
    "        scores_test = []\n",
    "        y_preds_train = []\n",
    "        y_preds_valid = []\n",
    "        y_preds_test = []\n",
    "\n",
    "        train_classes = []\n",
    "        valid_classes = []\n",
    "        test_classes = []\n",
    "        \n",
    "        mccs_train = []\n",
    "        mccs_valid = []\n",
    "        mccs_test = []\n",
    "        all_data = data['inputs']['all']\n",
    "        all_meta = data['meta']['all']\n",
    "        all_labels = data['labels']['all']\n",
    "        # all_names = data['names']['all']\n",
    "        all_batches = data['batches']['all']\n",
    "        all_cats = data['cats']['all']\n",
    "        print(f'Iteration: {self.iter}')\n",
    "        for h in range(5):\n",
    "            if groupkfold:\n",
    "                skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=h)\n",
    "                train_nums = np.arange(0, len(all_labels))\n",
    "                # Remove samples from unwanted batches\n",
    "                train_inds, valid_inds = skf.split(train_nums, all_labels, all_batches).__next__()\n",
    "            else:\n",
    "                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "                train_nums = np.arange(0, len(all_labels))\n",
    "                train_inds, valid_inds = skf.split(train_nums, all_labels).__next__()\n",
    "            train_data, valid_data = all_data.iloc[train_inds], all_data.iloc[valid_inds]\n",
    "            train_meta, valid_meta = all_meta.iloc[train_inds], all_meta.iloc[valid_inds]\n",
    "            train_labels, valid_labels = all_labels[train_inds], all_labels[valid_inds]\n",
    "            # train_names, valid_names = all_names[train_inds], all_names[valid_inds]\n",
    "            train_batches, valid_batches = all_batches[train_inds], all_batches[valid_inds]\n",
    "            train_cats, valid_cats = all_cats[train_inds], all_cats[valid_inds]\n",
    "\n",
    "            if groupkfold:\n",
    "                skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=h)\n",
    "                train_nums = np.arange(0, len(train_labels))\n",
    "                # Remove samples from unwanted batches\n",
    "                train_inds, test_inds = skf.split(train_nums, train_labels, train_batches).__next__()\n",
    "            else:\n",
    "                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "                train_nums = np.arange(0, len(train_labels))\n",
    "                train_inds, test_inds = skf.split(train_nums, train_labels).__next__()\n",
    "            train_data, test_data = train_data.iloc[train_inds], train_data.iloc[test_inds]\n",
    "            train_meta, test_meta = train_meta.iloc[train_inds], train_meta.iloc[test_inds]\n",
    "            train_labels, test_labels = train_labels[train_inds], train_labels[test_inds]\n",
    "            # train_names, test_names = train_names[train_inds], train_names[test_inds]\n",
    "            train_batches, test_batches = train_batches[train_inds], train_batches[test_inds]\n",
    "            train_cats, test_cats = train_cats[train_inds], train_cats[test_inds]\n",
    "\n",
    "            unique_labels = []\n",
    "            for l in train_labels:\n",
    "                if l not in unique_labels:\n",
    "                    unique_labels += [l]\n",
    "\n",
    "            unique_labels = np.array(unique_labels)\n",
    "            train_classes += [np.array([np.argwhere(l == unique_labels)[0][0] for l in train_labels])]\n",
    "            valid_classes += [np.array([np.argwhere(l == unique_labels)[0][0] for l in valid_labels])]\n",
    "            test_classes += [np.array([np.argwhere(l == unique_labels)[0][0] for l in test_labels])]\n",
    "\n",
    "\n",
    "            m = self.model()\n",
    "            m.set_params(**param_grid)\n",
    "            if self.ovr:\n",
    "                m = OneVsRestClassifier(m)\n",
    "            # try:\n",
    "            m.fit(train_data, train_classes[-1])\n",
    "            # except:\n",
    "            #     return 1\n",
    "\n",
    "            scores_valid += [m.score(valid_data, valid_classes[-1])]\n",
    "            scores_train += [m.score(train_data, train_classes[-1])]\n",
    "            scores_test += [m.score(test_data, test_classes[-1])]\n",
    "            # scores_train = score_train\n",
    "            # scores_valid = score_valid\n",
    "\n",
    "            y_preds_train += [m.predict(train_data)]\n",
    "            y_preds_valid += [m.predict(valid_data)]\n",
    "            y_preds_test += [m.predict(test_data)]\n",
    "\n",
    "            mccs_train += [MCC(train_classes[-1], y_preds_train[-1])]\n",
    "            mccs_valid += [MCC(valid_classes[-1], y_preds_valid[-1])]\n",
    "            mccs_test += [MCC(test_classes[-1], y_preds_test[-1])]\n",
    "            if self.best_scores_valid is None:\n",
    "                self.best_scores_valid = 0\n",
    "        print(self.best_scores_valid)\n",
    "        print('valid_score:', np.mean(scores_valid), 'h_params:', param_grid)\n",
    "        if np.mean(scores_valid) > np.mean(self.best_scores_valid):\n",
    "            self.best_scores_train = scores_train\n",
    "            self.best_scores_valid = scores_valid\n",
    "            self.best_scores_test = scores_test\n",
    "            self.best_mccs_train = mccs_train\n",
    "            self.best_mccs_valid = mccs_valid\n",
    "            self.best_mccs_test = mccs_test\n",
    "            fig = get_confusion_matrix(np.concatenate(train_classes), np.concatenate(y_preds_train), unique_labels)\n",
    "            save_confusion_matrix(fig, f\"{self.log_path}/confusion_matrices/{self.name}_{self.model_name}_train\", \n",
    "                                  acc=scores_train, mcc=mccs_train, group='train')\n",
    "            fig = get_confusion_matrix(np.concatenate(valid_classes), np.concatenate(y_preds_valid), unique_labels)\n",
    "            save_confusion_matrix(fig, f\"{self.log_path}/confusion_matrices/{self.name}_{self.model_name}_valid\", \n",
    "                                  acc=scores_valid, mcc=mccs_valid, group='valid')\n",
    "            fig = get_confusion_matrix(np.concatenate(test_classes), np.concatenate(y_preds_test), unique_labels)\n",
    "            save_confusion_matrix(fig, f\"{self.log_path}/confusion_matrices/{self.name}_{self.model_name}_test\", \n",
    "                                  acc=scores_test, mcc=mccs_test, group='test')\n",
    "            try:\n",
    "                self.best_roc_train = save_roc_curve(m, train_data, train_classes, unique_labels,\n",
    "                                                     f\"{self.log_path}/ROC/{self.name}_{self.model_name}_train\", binary=0,\n",
    "                                                     acc=score_train)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                self.best_roc_valid = save_roc_curve(m, valid_data, valid_classes, unique_labels,\n",
    "                                                     f\"{self.log_path}/ROC/{self.name}_{self.model_name}_valid\", binary=0,\n",
    "                                                     acc=score_valid)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                self.best_roc_test = save_roc_curve(m, test_data, test_classes, unique_labels,\n",
    "                                                     f\"{self.log_path}/ROC/{self.name}_{self.model_name}_test\", binary=0,\n",
    "                                                     acc=score_test)\n",
    "            except:\n",
    "                pass\n",
    "            self.save_best_model_hparams(self.hparams_names, param_grid)\n",
    "\n",
    "        return 1 - np.mean(scores_valid)\n",
    "\n",
    "    def save_best_model_hparams(self, hparams_names, params):\n",
    "        param_grid = {}\n",
    "        for name, param in zip(hparams_names, params):\n",
    "            param_grid[name] = param\n",
    "        self.best_params_dict = param_grid\n",
    "\n",
    "        self.best_params_dict_values['train_acc'] = self.best_scores_train\n",
    "        self.best_params_dict_values['valid_acc'] = self.best_scores_valid\n",
    "        self.best_params_dict_values['test_acc'] = self.best_scores_test\n",
    "        self.best_params_dict_values['train_acc'] = self.best_scores_train\n",
    "        self.best_params_dict_values['valid_acc'] = self.best_scores_valid\n",
    "        self.best_params_dict_values['test_acc'] = self.best_scores_test\n",
    "\n",
    "        self.best_params_dict_values['train_mcc'] = self.best_mccs_train\n",
    "        self.best_params_dict_values['valid_mcc'] = self.best_mccs_valid\n",
    "        self.best_params_dict_values['test_mcc'] = self.best_mccs_test\n",
    "        self.best_params_dict_values['train_mcc'] = self.best_mccs_train\n",
    "        self.best_params_dict_values['valid_mcc'] = self.best_mccs_valid\n",
    "        self.best_params_dict_values['test_mcc'] = self.best_mccs_test\n",
    "\n",
    "        self.best_params_dict['train_acc_mean'] = np.mean(self.best_scores_train)\n",
    "        self.best_params_dict['valid_acc_mean'] = np.mean(self.best_scores_valid)\n",
    "        self.best_params_dict['test_acc_mean'] = np.mean(self.best_scores_test)\n",
    "        self.best_params_dict['train_acc_std'] = np.std(self.best_scores_train)\n",
    "        self.best_params_dict['valid_acc_std'] = np.std(self.best_scores_valid)\n",
    "        self.best_params_dict['test_acc_std'] = np.std(self.best_scores_test)\n",
    "\n",
    "        self.best_params_dict['train_mcc_mean'] = np.mean(self.best_mccs_train)\n",
    "        self.best_params_dict['valid_mcc_mean'] = np.mean(self.best_mccs_valid)\n",
    "        self.best_params_dict['test_mcc_mean'] = np.mean(self.best_mccs_test)\n",
    "        self.best_params_dict['train_mcc_std'] = np.std(self.best_mccs_train)\n",
    "        self.best_params_dict['valid_mcc_std'] = np.std(self.best_mccs_valid)\n",
    "        self.best_params_dict['test_mcc_std'] = np.std(self.best_mccs_test)\n",
    "\n",
    "        print(self.log_path)\n",
    "        os.makedirs(f'{self.log_path}/saved_models/', exist_ok=True)\n",
    "        with open(f'{self.log_path}/saved_models/best_params_{self.name}_{self.model_name}.json', \"w\") as read_file:\n",
    "            json.dump(self.best_params_dict, read_file)\n",
    "        with open(f'{self.log_path}/saved_models/best_params_{self.name}_{self.model_name}_values.json', \"w\") as read_file:\n",
    "            json.dump(self.best_params_dict_values, read_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa988016-ab25-4483-8f17-b9a58872c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rfc(data, name, n_meta):\n",
    "    hparams_names = [x.name for x in rfc_space]\n",
    "    print(csv_name.split(\".\")[0])\n",
    "    train = Train(\"inputs\", RandomForestClassifier, data, hparams_names,\n",
    "                  f'results/unique_genes/{name}/rfc/{n_meta}', None, None, ovr=0, binary=False, mlops='None')\n",
    "    res = gp_minimize(train.train, rfc_space, n_calls=20, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "075816dd-38f3-4041-b2a8-57224b2bc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linsvc(data, name, n_meta):\n",
    "    hparams_names = [x.name for x in linsvc_space]\n",
    "    train = Train(\"inputs\", sklearn.svm.LinearSVC, data, hparams_names,\n",
    "                  f'results/unique_genes/{name}/linsvc/{n_meta}', None, None, ovr=0, binary=False, mlops='None')\n",
    "    res = gp_minimize(train.train, linsvc_space, n_calls=20, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827bdef7-2362-4ebc-bb69-79b99becce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def train_logreg(data, name, n_meta):\n",
    "    hparams_names = [x.name for x in logreg_space]\n",
    "    train = Train(\"inputs\", LogisticRegression, data, hparams_names,\n",
    "                  f'results/unique_genes/{name}/logreg/{n_meta}', None, None, ovr=0, binary=False, mlops='None')\n",
    "    res = gp_minimize(train.train, linsvc_space, n_calls=20, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7099c378-c954-4eb4-b137-78b2a566500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rLISI(data, meta_data, perplexity=10):\n",
    "    import rpy2.robjects as robjects\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects.packages import importr\n",
    "    from rpy2.robjects.conversion import localconverter\n",
    "    lisi = importr('lisi')\n",
    "    # all_batches_r = robjects.IntVector(all_batches[all_ranks])\n",
    "    # all_data_r.colnames = robjects.StrVector([str(x) for x in range(df.shape[1])])\n",
    "    # labels = ['label1', 'label2']\n",
    "    labels = robjects.StrVector(['label1'])\n",
    "    new_meta_data = robjects.r.matrix(robjects.IntVector(meta_data), nrow=data.shape[0])\n",
    "    newdata = robjects.r.matrix(robjects.FloatVector(data.values.reshape(-1)), nrow=data.shape[0])\n",
    "\n",
    "    new_meta_data.colnames = labels\n",
    "    results = lisi.compute_lisi(newdata, new_meta_data, labels, perplexity)\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        results = np.array(robjects.conversion.rpy2py(results))\n",
    "    mean = np.mean(results)\n",
    "    return mean  # , np.std(results), results\n",
    "def rKBET(inputs, cats):\n",
    "    kbet = importr('kBET')\n",
    "    # all_batches_r = robjects.IntVector(all_batches[all_ranks])\n",
    "    # all_data_r.colnames = robjects.StrVector([str(x) for x in range(df.shape[1])])\n",
    "    # labels = ['label1', 'label2']\n",
    "    labels = robjects.StrVector(['label1'])\n",
    "    new_meta_data = robjects.IntVector(cats)\n",
    "    newdata = robjects.r.matrix(robjects.FloatVector(inputs.values.reshape(-1)), nrow=inputs.shape[0])\n",
    "\n",
    "    new_meta_data.colnames = labels\n",
    "    results = kbet.kBET(newdata, new_meta_data, do_pca=False, plot=False)\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        results = robjects.conversion.rpy2py(results[0])\n",
    "    try:\n",
    "        mean = results['kBET.signif'][0]\n",
    "    except:\n",
    "        mean = 0\n",
    "\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36c3425a-287f-4766-a98c-164f711fed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "def make_pval_table(data, unique_labels):\n",
    "    print(\"Mann      pval min    n pvals < 0.05\")\n",
    "    table = pd.DataFrame(columns=['pval', 'n'])\n",
    "    i = 0\n",
    "    for i, label in enumerate(unique_labels[:-1]):\n",
    "        for label2 in unique_labels[i+1:]:\n",
    "            if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "                pvals = stats.mannwhitneyu(\n",
    "                    data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                    data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "                )\n",
    "                tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "                table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "                table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "                i += 1\n",
    "    print(tabulate(table))\n",
    "\n",
    "    print('ttests')\n",
    "    table = pd.DataFrame(columns=['pval'])\n",
    "    i = 0\n",
    "    for i, label in enumerate(unique_labels[:-1]):\n",
    "        for label2 in unique_labels[i+1:]:\n",
    "            if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "                pvals = stats.ttest_ind(\n",
    "                    data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                    data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "                )\n",
    "                tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "                table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "                table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "                i += 1\n",
    "    print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7aabd9b-18b0-45e1-9285-b7745865836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# from src.dl.models.pytorch.utils.metrics import rLISI\n",
    "def log_fct(data, unique_labels, unique_batches, metrics):\n",
    "    make_pval_table(data, unique_labels)\n",
    "    if log_stuff:\n",
    "        # metrics, _ = pyGPCA(data, 'all_pool', 'pool_raw', metrics)\n",
    "        # metrics, _ = pyGPCA(data, 'all', 'raw', metrics)\n",
    "        # metrics = log_pool_metrics(data['inputs'], data['batches'], data['labels'], unique_labels, None, 0, metrics, 'raw', mlops=None)\n",
    "        metrics = log_metrics(data, unique_labels, data['batches'], data['labels'], unique_labels, metrics, 'raw', device='cuda')\n",
    "        # metrics = log_LDA(LDA, data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'raw')\n",
    "    log_ORD(ordin={'model': PCA(n_components=2), 'name': f'PCA_inputs'}, logger=None, data=data, \n",
    "            uniques={'batches': unique_batches, 'labels': unique_labels}, mlops=None, epoch=0)\n",
    "    log_ORD(ordin={'model': UMAP(n_components=2), 'name': f'UMAP_inputs'}, logger=None, data=data, \n",
    "            uniques={'batches': unique_batches, 'labels': unique_labels}, mlops=None, epoch=0)\n",
    "    return metrics\n",
    "\n",
    "def train_fct(data, n_meta, train_models):\n",
    "    if n_meta == 2:\n",
    "        for group in data['inputs']:\n",
    "            data['inputs'][group] = pd.concat((data['inputs'][group], data['meta'][group]), 1)\n",
    "\n",
    "    if train_models:\n",
    "        train_linsvc(data, 'raw', n_meta)\n",
    "        train_rfc(data, 'raw', n_meta)\n",
    "        # train_logreg(data, 'raw', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2adb76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_ORD(ordin, data, uniques, epoch, transductive=True):\n",
    "    model = ordin['model']\n",
    "    data = copy.deepcopy(data)\n",
    "    for f in ['inputs', 'batches', 'labels']:\n",
    "        for g in ['train', 'valid', 'test']:\n",
    "            print(f\"{f} {g}\")\n",
    "    if transductive:\n",
    "        model.fit(np.concatenate((data['inputs']['train'], data['inputs']['valid'], data['inputs']['test'])))\n",
    "        pcs_train = model.transform(data['inputs']['train'])\n",
    "        if \"transductive\" not in ordin['name']:\n",
    "            ordin['name'] += \"_transductive\"\n",
    "    else:\n",
    "        pcs_train = model.fit_transform(data['inputs']['train'])\n",
    "    if data['inputs']['valid'] is not None:\n",
    "        pcs_valid = model.transform(data['inputs']['valid'])\n",
    "    else:\n",
    "        pcs_valid = np.array([])\n",
    "    if data['inputs']['test'] is not None:\n",
    "        pcs_test = model.transform(data['inputs']['test'])\n",
    "    else:\n",
    "        pcs_test = np.array([])\n",
    "        test_labels = np.array([])\n",
    "    pcs_train_df = pd.DataFrame(data=pcs_train, columns=['PC 1', 'PC 2'])\n",
    "    pcs_valid_df = pd.DataFrame(data=pcs_valid, columns=['PC 1', 'PC 2'])\n",
    "    pcs_test_df = pd.DataFrame(data=pcs_test, columns=['PC 1', 'PC 2'])\n",
    "    try:\n",
    "        ev = model.explained_variance_ratio_\n",
    "        pc1 = 'Component_1 : ' + str(np.round(ev[0] * 100, decimals=2)) + \"%\"\n",
    "        pc2 = 'Component_2 : ' + str(np.round(ev[1] * 100, decimals=2)) + \"%\"\n",
    "    except:\n",
    "        pc1 = 'Component_1'\n",
    "        pc2 = 'Component_2'\n",
    "\n",
    "    for name in list(uniques.keys()):\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_xlabel(pc1, fontsize=20)\n",
    "        ax.set_ylabel(pc2, fontsize=20)\n",
    "        # ax.set_title(f\"2 component {ordin['name']} {name}\", fontsize=20)\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        num_targets = len(uniques[name])\n",
    "        cmap = plt.cm.tab20\n",
    "\n",
    "        cols = cmap(np.linspace(0, 1, len(uniques[name]) + 1))\n",
    "        colors = rcParams['axes.prop_cycle'] = cycler(color=cols)\n",
    "        colors_list = {name: [] for name in ['train', 'valid', 'test']}\n",
    "        data1_list = {name: [] for name in ['train', 'valid', 'test']}\n",
    "        data2_list = {name: [] for name in ['train', 'valid', 'test']}\n",
    "        new_labels = {name: [] for name in ['train', 'valid', 'test']}\n",
    "        new_cats = {name: [] for name in ['train', 'valid', 'test']}\n",
    "\n",
    "        ellipses = []\n",
    "        unique_cats_train = np.array([])\n",
    "        for df_name, df, labels in zip(['train', 'valid', 'test'],\n",
    "                                       [pcs_train_df, pcs_valid_df, pcs_test_df],\n",
    "                                       [data[name]['train'], data[name]['valid'], data[name]['test']]):\n",
    "            for t, target in enumerate(uniques[name]):\n",
    "                indices_to_keep = [True if x == target else False for x in list(labels)]\n",
    "                data1 = list(df.loc[indices_to_keep, 'PC 1'])\n",
    "                new_labels[df_name] += [target for _ in range(len(data1))]\n",
    "                new_cats[df_name] += [target for _ in range(len(data1))]\n",
    "\n",
    "                data2 = list(df.loc[indices_to_keep, 'PC 2'])\n",
    "                data1_list[df_name] += [data1]\n",
    "                data2_list[df_name] += [data2]\n",
    "                colors_list[df_name] += [np.array([[cols[t]] * len(data1)])]\n",
    "                if len(indices_to_keep) > 1 and df_name == 'train_data' or target not in unique_cats_train:\n",
    "                    unique_cats_train = np.unique(np.concatenate((new_labels[df_name], unique_cats_train)))\n",
    "                    try:\n",
    "                        confidence_ellipses = confidence_ellipse(np.array(data1), np.array(data2), ax, 1.5,\n",
    "                                                                 edgecolor=cols[t],\n",
    "                                                                 train_set=True)\n",
    "                        ellipses += [confidence_ellipses[1]]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        for df_name, marker in zip(list(data1_list.keys()), ['o', 'x', '*']):\n",
    "            data1_vector = np.hstack([d for d in data1_list[df_name] if len(d) > 0]).reshape(-1, 1)\n",
    "            colors_vector = np.hstack([d for d in colors_list[df_name] if d.shape[1] > 0]).squeeze()\n",
    "            data2_vector = np.hstack(data2_list[df_name]).reshape(-1, 1)\n",
    "            data_colors_vector = np.concatenate((data1_vector, data2_vector, colors_vector), axis=1)\n",
    "            data2 = data_colors_vector[:, 1]\n",
    "            col = data_colors_vector[:, 2:]\n",
    "            data1 = data_colors_vector[:, 0]\n",
    "\n",
    "            ax.scatter(data1, data2, s=50, alpha=1.0, c=col, label=new_labels[df_name], marker=marker)\n",
    "            custom_lines = [Line2D([0], [0], color=cmap(x), lw=4) for x in np.linspace(0, 1, len(uniques[name]) + 1)]\n",
    "            if 'pool' in uniques[name]:\n",
    "                uniques[name][np.argwhere(uniques[name] == 'pool')] = 'QC'\n",
    "            ax.legend(custom_lines, uniques[name].tolist(), fontsize=10)\n",
    "            # print(data1)\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def log_LDA(ordin, data, uniques, epoch, metrics, model_name):\n",
    "    for name in list(uniques.keys()):\n",
    "        skf = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "        train_nums = np.arange(0, len(data['labels']['all']))\n",
    "        scores = []\n",
    "        # Remove samples from unwanted batches\n",
    "        for i, (train_inds, valid_inds) in enumerate(skf.split(train_nums, data['labels']['all'], data['batches']['all'])):\n",
    "\n",
    "            if len(uniques[name]) > 2:\n",
    "                n_comp = 2\n",
    "            else:\n",
    "                n_comp = 1\n",
    "            model = ordin(n_components=n_comp)\n",
    "            \n",
    "            pcs_train = model.fit_transform(data['inputs']['all'][train_inds], data[name]['all'][train_inds])\n",
    "            pcs_valid = model.transform(data['inputs']['all'][valid_inds])\n",
    "            scores += [model.score(data['inputs']['all'][valid_inds], data[name]['all'][valid_inds])]\n",
    "            if i == 0:\n",
    "                fig = plt.figure(figsize=(12, 12))\n",
    "                ax = fig.add_subplot(111)\n",
    "                # pcs_train = model.transform(np.concatenate((data['inputs']['train'], data['inputs']['train_pool']), 0))\n",
    "                # pcs_valid = model.transform(np.concatenate((data['inputs']['valid'], data['inputs']['valid_pool']), 0))\n",
    "                # pcs_test = model.transform(np.concatenate((data['inputs']['test'], data['inputs']['test_pool']), 0))\n",
    "\n",
    "                if n_comp > 1:\n",
    "                    pcs_train_df = pd.DataFrame(data=pcs_train, columns=['LD1', 'LD2'])\n",
    "                    pcs_valid_df = pd.DataFrame(data=pcs_valid, columns=['LD1', 'LD2'])\n",
    "                    # pcs_test_df = pd.DataFrame(data=pcs_test, columns=['LD1', 'LD2'])\n",
    "                else:\n",
    "                    pcs_train_df = pd.DataFrame(data=pcs_train, columns=['LD1'])\n",
    "                    pcs_valid_df = pd.DataFrame(data=pcs_valid, columns=['LD1'])\n",
    "                    # pcs_test_df = pd.DataFrame(data=pcs_test, columns=['LD1'])\n",
    "\n",
    "                try:\n",
    "                    ev = model.explained_variance_ratio_\n",
    "                    pc1 = 'Component_1 : ' + str(np.round(ev[0] * 100, decimals=2)) + \"%\"\n",
    "                    pc2 = 'Component_2 : ' + str(np.round(ev[1] * 100, decimals=2)) + \"%\"\n",
    "                except:\n",
    "                    pc1 = 'Component_1'\n",
    "                    pc2 = 'Component_2'\n",
    "\n",
    "                ax.set_xlabel(pc1, fontsize=15)\n",
    "                ax.set_ylabel(pc2, fontsize=15)\n",
    "                ax.set_title(f\"2 component LDA\", fontsize=20)\n",
    "\n",
    "                num_targets = len(uniques[name])\n",
    "                cmap = plt.cm.tab20\n",
    "\n",
    "                cols = cmap(np.linspace(0, 1, len(uniques[name]) + 1))\n",
    "                colors = rcParams['axes.prop_cycle'] = cycler(color=cols)\n",
    "                colors_list = {name: [] for name in ['train_data', 'valid_data']}\n",
    "                data1_list = {name: [] for name in ['train_data', 'valid_data']}\n",
    "                data2_list = {name: [] for name in ['train_data', 'valid_data']}\n",
    "                new_labels = {name: [] for name in ['train_data', 'valid_data']}\n",
    "                new_cats = {name: [] for name in ['train_data', 'valid_data']}\n",
    "\n",
    "                ellipses = []\n",
    "                unique_cats_train = np.array([])\n",
    "\n",
    "                for df_name, df, labels in zip(['train_data', 'valid_data'],\n",
    "                                               [pcs_train_df, pcs_valid_df],\n",
    "                                               [data[name]['all'][train_inds], data[name]['all'][valid_inds]]):\n",
    "                    for t, target in enumerate(uniques[name]):\n",
    "                        indices_to_keep = [True if x == target else False for x in\n",
    "                                           list(labels)]  # 0 is the name of the column with target values\n",
    "                        data1 = list(df.loc[indices_to_keep, 'LD1'])\n",
    "                        new_labels[df_name] += [target for _ in range(len(data1))]\n",
    "                        new_cats[df_name] += [target for _ in range(len(data1))]\n",
    "\n",
    "                        data1_list[df_name] += [data1]\n",
    "                        colors_list[df_name] += [np.array([[cols[t]] * len(data1)])]\n",
    "                        if n_comp > 1:\n",
    "                            data2 = list(df.loc[indices_to_keep, 'LD2'])\n",
    "                            data2_list[df_name] += [data2]\n",
    "                        # if len(indices_to_keep) > 1 and df_name == 'all_data' or target not in unique_cats_train:\n",
    "                        unique_cats_train = np.unique(np.concatenate((new_labels[df_name], unique_cats_train)))\n",
    "                        if n_comp > 1:\n",
    "                            try:\n",
    "                                confidence_ellipses = confidence_ellipse(np.array(data1), np.array(data2), ax, 1.5,\n",
    "                                                                         edgecolor=cols[t],\n",
    "                                                                         train_set=True)\n",
    "                                ellipses += [confidence_ellipses[1]]\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                for df_name, marker in zip(list(data1_list.keys()), ['o', '*']):\n",
    "                    data1_vector = np.hstack([d for d in data1_list[df_name] if len(d) > 0]).reshape(-1, 1)\n",
    "                    colors_vector = np.hstack([d for d in colors_list[df_name] if d.shape[1] > 0]).squeeze()\n",
    "                    if n_comp > 1:\n",
    "                        data2_vector = np.hstack(data2_list[df_name]).reshape(-1, 1)\n",
    "                        data_colors_vector = np.concatenate((data1_vector, data2_vector, colors_vector), axis=1)\n",
    "                        data1 = data_colors_vector[:, 0]\n",
    "                        data2 = data_colors_vector[:, 1]\n",
    "                        col = data_colors_vector[:, 2:]\n",
    "                        ax.scatter(data1, data2, s=50, alpha=1.0, c=col, label=new_labels[df_name], marker=marker)\n",
    "                    else:\n",
    "                        data_colors_vector = np.concatenate((data1_vector, colors_vector), axis=1)\n",
    "                        data1 = data_colors_vector[:, 0]\n",
    "                        col = data_colors_vector[:, 1:]\n",
    "                        ax.scatter(data1, np.random.random(len(data1)), s=50, alpha=1.0, c=col, label=new_labels[df_name],\n",
    "                                   marker=marker)\n",
    "\n",
    "                    custom_lines = [Line2D([0], [0], color=cmap(x), lw=4) for x in np.linspace(0, 1, len(uniques[name]) + 1)]\n",
    "                    ax.legend(custom_lines, uniques[name].tolist())\n",
    "\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                plt.close()\n",
    "        metrics[model_name][f'LDA_score_{name}'] = np.mean(scores)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c8cc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def get_metrics(data, metrics, form):\n",
    "    # sets are grouped togheter for a single metric\n",
    "    knns = {repres: KNeighborsClassifier(n_neighbors=20) for repres in ['domains', 'labels']}\n",
    "    # values = {group: {m : {'labels': [], 'domains': []} for m in ['lisi', 'kbet', 'silhouette', 'adjusted_rand_score', 'adjusted_mutual_info_score']} \n",
    "    #           for group in ['train', 'valid', 'test', 'all', 'all_pool', 'train_pool', 'valid_pool', 'test_pool']}\n",
    "\n",
    "    if form not in metrics:\n",
    "        metrics[form] = {}\n",
    "    for group in ['all']:\n",
    "        # metrics[group] = {m : {'labels': [], 'domains': []} for m in ['lisi', 'kbet', 'silhouette', 'adjusted_rand_score', 'adjusted_mutual_info_score']} \n",
    "        # print(group)\n",
    "        if group not in metrics[form]:\n",
    "            metrics[form][group] = {}\n",
    "\n",
    "        if group == 'all':\n",
    "            knns['domains'].fit(data['inputs'][group], data['batches'][group])\n",
    "            knns['labels'].fit(data['inputs'][group], data['cats'][group])\n",
    "        if 'pool' not in group or 'all_pool' == group:\n",
    "            for metric, funct in zip(['lisi', 'silhouette', 'kbet'], [rLISI, silhouette_score, rKBET]):\n",
    "                metrics[form][group][metric] = {'labels': None, 'domains': None}\n",
    "                metrics[form][group][metric]['domains'] = funct(data['inputs'][group], data['batches'][group])\n",
    "                if 'pool' not in group:\n",
    "                    metrics[form][group][metric]['labels'] = funct(data['inputs'][group], data['cats'][group])\n",
    "\n",
    "            domain_preds = knns['domains'].predict(data['inputs'][group])\n",
    "            metrics[form][group]['shannon'] = {'labels': None, 'domains': None}\n",
    "            # print(knns['domains'].predict_proba(data['inputs'][group]))\n",
    "            metrics[form][group]['shannon']['domains'] = batch_entropy(knns['domains'].predict_proba(data['inputs'][group]))\n",
    "            if 'pool' not in group:\n",
    "                labels_preds = knns['labels'].predict(data['inputs'][group])\n",
    "                metrics[form][group]['shannon']['labels'] = batch_entropy(knns['labels'].predict_proba(data['inputs'][group]))\n",
    "\n",
    "            for metric, funct in zip(\n",
    "                    ['adjusted_rand_score', 'adjusted_mutual_info_score'],\n",
    "                    [adjusted_rand_score, adjusted_mutual_info_score]):\n",
    "                metrics[form][group][metric] = {'labels': None, 'domains': None}\n",
    "                metrics[form][group][metric]['domains'] = funct(data['batches'][group], domain_preds)\n",
    "                if 'pool' not in group:\n",
    "                    metrics[form][group][metric]['labels'] = funct(data['batches'][group], labels_preds)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bca259c1-b26b-453b-b8ed-8ab2217a02af",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125a8b8-8a42-4cf3-be6d-50290229b4e5",
   "metadata": {},
   "source": [
    "# RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be2df2d5-2f54-4b19-96e6-2b6f0c519b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "# unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "# matrix = pd.read_csv(\n",
    "#     f\"{path}/{csv_name}\", sep=','\n",
    "# )\n",
    "# names = [x.split(\"\\\\\")[-1].split('-')[1:2] for x in list(matrix.columns)[1:] if 'Pool' not in x]\n",
    "# list(matrix.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3679330e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "log_metrics() missing 2 required positional arguments: 'mlops' and 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb Cell 23\u001b[0m line \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloggings\u001b[39;00m \u001b[39mimport\u001b[39;00m get_metrics, log_metrics, batch_f1_score\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m metrics \u001b[39m=\u001b[39m log_metrics(data, unique_labels, data[\u001b[39m'\u001b[39;49m\u001b[39mbatches\u001b[39;49m\u001b[39m'\u001b[39;49m], data[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m], unique_labels, metrics, \u001b[39m'\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m'\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: log_metrics() missing 2 required positional arguments: 'mlops' and 'metrics'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann      pval min    n pvals < 0.05\n",
      "-----  -  -\n",
      "HF_CD  0  1\n",
      "-----  -  -\n",
      "ttests\n",
      "-----  -----------  ----\n",
      "HF_CD  7.43101e-34  2369\n",
      "-----  -----------  ----\n"
     ]
    }
   ],
   "source": [
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b9c28-9226-4a89-afe2-41b2111132e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann      pval min    n pvals < 0.05\n",
      "-----  -  -\n",
      "HF_CD  0  1\n",
      "-----  -  -\n",
      "ttests\n",
      "-----  -----------  ----\n",
      "HF_CD  7.43101e-34  2369\n",
      "-----  -----------  ----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb Cell 24\u001b[0m line \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m metrics \u001b[39m=\u001b[39m log_fct(copy\u001b[39m.\u001b[39;49mdeepcopy(data), unique_labels, unique_batches, metrics)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_fct(copy\u001b[39m.\u001b[39mdeepcopy(data), n_meta, train_models)\n",
      "\u001b[1;32m/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb Cell 24\u001b[0m line \u001b[0;36mlog_fct\u001b[0;34m(data, unique_labels, unique_batches, metrics)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     metrics \u001b[39m=\u001b[39m log_metrics(data, unique_labels, data[\u001b[39m'\u001b[39m\u001b[39mbatches\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m], unique_labels, metrics, \u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# metrics = log_LDA(LDA, data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'raw')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m log_ORD(ordin\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m: PCA(n_components\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPCA_inputs\u001b[39;49m\u001b[39m'\u001b[39;49m}, logger\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, data\u001b[39m=\u001b[39;49mdata, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         uniques\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mbatches\u001b[39;49m\u001b[39m'\u001b[39;49m: unique_batches, \u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m: unique_labels}, mlops\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m log_ORD(ordin\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m: UMAP(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUMAP_inputs\u001b[39m\u001b[39m'\u001b[39m}, logger\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39mdata, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         uniques\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mbatches\u001b[39m\u001b[39m'\u001b[39m: unique_batches, \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: unique_labels}, mlops\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, epoch\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.3.33/home/simonp/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/notebooks/batch_correction-mice2.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m/mnt/projects_tn01/AD_Bacteria/GITLAB/BERNN_MSMS/src/dl/models/pytorch/utils/loggings.py:942\u001b[0m, in \u001b[0;36mlog_ORD\u001b[0;34m(ordin, logger, data, uniques, mlops, epoch, transductive)\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[39mfor\u001b[39;00m df_name, marker \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mlist\u001b[39m(data1_list\u001b[39m.\u001b[39mkeys()), [\u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m--> 942\u001b[0m     data1_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhstack([d \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m data1_list[df_name] \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(d) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    943\u001b[0m     colors_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack([d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m colors_list[df_name] \u001b[39mif\u001b[39;00m d\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m])\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m    944\u001b[0m     data2_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack(data2_list[df_name])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39mconcatenate(arrs, \u001b[39m0\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea420a-4bc1-4924-9b35-6988abbb6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    # table = pd.DataFrame(columns=list(metrics['raw']['all'].keys()) + ['delta', 'delta_pool'], index=list(metrics.keys()))\n",
    "    cols = ['qc_aPCC', '[qc_dist/tot_eucl]', 'lisi', 'silhouette', 'kbet', 'shannon', 'adjusted_rand_score', 'adjusted_mutual_info_score']\n",
    "    cols_pool = ['pool lisi', 'pool silhouette', 'pool kbet', 'pool shannon', 'pool adjusted_rand_score', 'pool adjusted_mutual_info_score',]\n",
    "    table = pd.DataFrame(columns=cols + cols_pool, index=list(metrics.keys()))\n",
    "    # table = table.drop(['qc_dist', 'b_euclidean', 'euclidean', \"[b_euclidean/tot_eucl]\"], 1)\n",
    "    for col in cols:\n",
    "        for row in list(table.index):\n",
    "            # if 'delta' in col:\n",
    "            #     table[col][row] = metrics[row][col]\n",
    "            # else:\n",
    "            try:\n",
    "                if isinstance(metrics[row]['all'][col], dict):\n",
    "                    table[col][row] = metrics[row]['all'][col][\"domains\"]\n",
    "                    try:\n",
    "                        table[f'pool {col}'][row] = metrics[row]['all_pool'][col][\"domains\"]\n",
    "                    except:\n",
    "                        pass\n",
    "                    # table[col][row][\"domains\"] = metrics[row]['all'][col][\"domains\"]\n",
    "                    # table[col][row][\"labels\"] = metrics[row]['all'][col][\"labels\"]\n",
    "                else:\n",
    "                    try:\n",
    "                        table[col][row] = metrics[row]['valid'][col]\n",
    "                    except:\n",
    "                        table[col][row] = metrics[row]['all'][col]\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    table[col][row] = metrics[row]['all_pool'][col]\n",
    "                except:\n",
    "                    table[col][row] = metrics[row]['valid'][col]\n",
    "\n",
    "    table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95666c8d-813a-44c4-acf4-acc44e64be80",
   "metadata": {},
   "source": [
    "# meta only"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6676e705-22f1-49f0-8667-217a6153a518",
   "metadata": {},
   "source": [
    "for group in data['inputs']:\n",
    "    data['inputs'][group] = data['meta'][group]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63a62e77-91fc-4f3e-bec9-196c38be6058",
   "metadata": {},
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_inputs_labels'}, data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "        # 'age': unique_ages, 'gender': unique_genders\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f11590b-ac67-487a-bbbc-f11de90909a9",
   "metadata": {},
   "source": [
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_inputs_labels'}, data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages, 'gender': unique_genders}, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e98e22-4203-4846-8d2c-f53b1f68459d",
   "metadata": {},
   "source": [
    "# Minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd8b33-ed1c-46fb-ae11-8a944fe378d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('minmax', data, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf7855-934c-4b69-8704-f9910595eb9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cf0cf-1d73-4fcf-ba52-5d4516bea34d",
   "metadata": {},
   "source": [
    "# Minmax per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ced76-c018-4a97-b4a0-cb13b9f73f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('minmax_per_batch', data, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88990bc7-af1c-4dc8-aad6-b7378efa73fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d298c-b19a-4b93-a39a-8a87f5e9cffd",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47bcc77-f1dc-4031-abb4-f9e9d06c6820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac98c00-641c-4450-8a05-922ce7d6c8c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d3aca-07d5-4df2-92fd-3a17c32aed26",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Standard per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72749a-5149-4529-8884-635074b31b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard_per_batch', data, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e1ddc-3206-471a-8ad7-c9470e87b9a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4105e1-591a-43c4-8069-bf26642f2a14",
   "metadata": {},
   "source": [
    "# Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a0f06-837e-4b1b-bc88-8620d0c3a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('robust', data, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cfce9a-15ff-487e-a25b-0cba7e7005a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78827d0f-e01c-4056-8178-e8967675e9fa",
   "metadata": {},
   "source": [
    "# Robust per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e523a-b86b-45b3-a936-b9e5a73131fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('robust_per_batch', data, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd47c0d-5b97-4cd5-ba71-07701bb49cdf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a3dde-1b9a-4d8d-aea4-ab7d060875a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7829bf-154f-4978-8baa-e768345de54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data = remove_batch_effect(get_berm('combat'), data)\n",
    "\n",
    "# data['inputs']['all'] = data['inputs']['all'].fillna(0)\n",
    "#data['inputs']['train'][np.isnan(data['inputs']['train'])] = 0\n",
    "# data['inputs']['train_pool'][np.isnan(data['inputs']['train_pool'])] = 0\n",
    "# data['inputs']['valid'][np.isnan(data['inputs']['valid'])] = 0\n",
    "# data['inputs']['valid_pool'][np.isnan(data['inputs']['valid_pool'])] = 0\n",
    "# data['inputs']['test'][np.isnan(data['inputs']['test'])] = 0\n",
    "# data['inputs']['test_pool'][np.isnan(data['inputs']['test_pool'])] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822054a-16ce-47ec-be3e-97f49c3d8ae5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93871c-7927-4d37-bcf0-172b1af0458a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pyCombat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275bc4e-1133-445c-a24f-4bde7937e15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from combat.pycombat import pycombat\n",
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data = use_pycombat(pycombat, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38771054-a15b-4995-b172-7c6548476e90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c1508f-2958-4d95-bcf1-c60a4c2d2e4c",
   "metadata": {},
   "source": [
    "# Harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb63fd8-c94d-4491-97c9-27133e41fa35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data['inputs']['all'].iloc[:] = minmax_scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'].iloc[:] = minmax_scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'].iloc[:] = minmax_scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'].iloc[:] = minmax_scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'].iloc[:] = minmax_scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'].iloc[:] = minmax_scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'].iloc[:] = minmax_scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'].iloc[:] = minmax_scaler.transform(data['inputs']['test_pool'])\n",
    "data = remove_batch_effect(get_berm('harmony'), data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f04940-b9d8-4d19-b926-711ce52deb52",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638c638-26f8-4997-abb1-c80a8b62e4c1",
   "metadata": {},
   "source": [
    "# WaveICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702d406-3c15-4be5-adf5-b34a4afd2861",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "def waveicaR(data, batches):\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        data_r = robjects.conversion.py2rpy(data)\n",
    "\n",
    "    # data_r = robjects.r.matrix(robjects.FloatVector(df.values.reshape(-1)), nrow=df.shape[0])\n",
    "    batches_r = robjects.IntVector(batches.reshape(-1))\n",
    "    waveica = importr('WaveICA')\n",
    "    # data_r.colnames = robjects.StrVector([str(x) for x in range(df.shape[1])])\n",
    "    newdata = waveica.WaveICA(dat=data_r, batch=batches_r)\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        newdata = { key : np.array(robjects.conversion.rpy2py(newdata.rx2(key))) for key in newdata.names }\n",
    "    return newdata['data_wave']\n",
    "\n",
    "\n",
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "df = data['inputs']['all']\n",
    "all_batches = data['batches']['all']\n",
    "# assert np.sum(all_batches != np.concatenate((data['batches']['train'], data['batches']['valid'], data['batches']['test']))) == 0\n",
    "tmp = waveicaR(df, all_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d096024-8483-4877-9445-b776bc5b5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_len = 0\n",
    "for g in list(data['inputs'].keys())[1:-1]:\n",
    "    data['inputs'][g] = pd.DataFrame(\n",
    "        tmp[previous_len:previous_len + data['inputs'][g].shape[0]],\n",
    "        index=data['inputs'][g].index)\n",
    "    previous_len += data['inputs'][g].shape[0]\n",
    "try:\n",
    "    data['inputs']['all'] = pd.DataFrame(tmp, index=df.index, columns=df.columns)\n",
    "except:\n",
    "    data['inputs']['all'] = pd.DataFrame(tmp, index=df.index)\n",
    "\n",
    "if n_meta == 2:\n",
    "    for group in data['inputs']:\n",
    "        data['inputs'][group] = pd.DataFrame(np.concatenate((data['inputs'][group], data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25860b7-9671-492e-9392-fd07f2564f29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = log_fct(copy.deepcopy(data), unique_labels, unique_batches, metrics)\n",
    "train_fct(copy.deepcopy(data), n_meta, train_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108f06c-37db-4228-8303-a229cd91103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    # table = pd.DataFrame(columns=list(metrics['raw']['all'].keys()) + ['delta', 'delta_pool'], index=list(metrics.keys()))\n",
    "    cols = ['qc_aPCC', '[qc_dist/tot_eucl]', 'lisi', 'silhouette', 'kbet', 'shannon', 'adjusted_rand_score', 'adjusted_mutual_info_score']\n",
    "    cols_pool = ['pool lisi', 'pool silhouette', 'pool kbet', 'pool shannon', 'pool adjusted_rand_score', 'pool adjusted_mutual_info_score',]\n",
    "    table = pd.DataFrame(columns=cols + cols_pool, index=list(metrics.keys()))\n",
    "    # table = table.drop(['qc_dist', 'b_euclidean', 'euclidean', \"[b_euclidean/tot_eucl]\"], 1)\n",
    "    for col in cols:\n",
    "        for row in list(table.index):\n",
    "            # if 'delta' in col:\n",
    "            #     table[col][row] = metrics[row][col]\n",
    "            # else:\n",
    "            try:\n",
    "                if isinstance(metrics[row]['all'][col], dict):\n",
    "                    table[col][row] = metrics[row]['all'][col][\"domains\"]\n",
    "                    try:\n",
    "                        table[f'pool {col}'][row] = metrics[row]['all_pool'][col][\"domains\"]\n",
    "                    except:\n",
    "                        pass\n",
    "                    # table[col][row][\"domains\"] = metrics[row]['all'][col][\"domains\"]\n",
    "                    # table[col][row][\"labels\"] = metrics[row]['all'][col][\"labels\"]\n",
    "                else:\n",
    "                    try:\n",
    "                        table[col][row] = metrics[row]['valid'][col]\n",
    "                    except:\n",
    "                        table[col][row] = metrics[row]['all'][col]\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    table[col][row] = metrics[row]['all_pool'][col]\n",
    "                except:\n",
    "                    table[col][row] = metrics[row]['valid'][col]\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d443349-8264-4af3-b6a5-dae17f36b17a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412abb89-ffe8-49ee-b774-0874dc5d8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dl.models.pytorch.aedann import AutoEncoder2 as AutoEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76adf46b-97d0-4011-9a84-500d30587ebf",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20491d29-7b08-4ace-a253-020a0254b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 896)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['inputs']['all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720cef8-cb9f-46fe-9164-4a12cb64dbdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12588\\1012383495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# best score is autoencoder0, best correction autoencoder3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbest_correction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n\u001b[0m\u001b[0;32m     42\u001b[0m                      \u001b[0mn_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                      \u001b[0mnb_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('robust', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/no_vae0/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=1692,\n",
    "                     mapper=0,\n",
    "                     layer2=183,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=0, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87795be-1119-4dff-a199-be26159fc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False, mapping=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False, mapping=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False, mapping=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False, mapping=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba03d4e-3fb9-4fc0-9da0-a1d8dcb024da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5bfb34-4183-4f66-a223-d0568767dea3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'ae-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'ae-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'ae-enc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62094998-3706-4525-8695-9f2730321b7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0103cf2b-683e-44e4-b684-2c921d6551c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e7376-4852-4fdd-9109-809038b5969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group].values, enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a3193-56f4-489c-9e84-7012b25e8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'ae-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'ae-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'ae-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3de76e81-cb30-40d6-9710-0f0ba0fa31a1",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a775f86-3e67-46d5-8d9e-008380af3eae",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c017337d-07dd-485b-82e8-bc9090e0828f",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae8baae9-81ae-4714-801d-460e30921f43",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb734b11-50ff-41d9-be27-eacfb3b7b89a",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef7ade47-b0a2-413d-b5f4-5d92815c9e44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8aa404fb-b5f0-4428-a481-8bab55e11586",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'ae-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'ae-rec', device='cuda')\n",
    "    metrics =log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'ae-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee1f99b3-e679-42a1-ad81-e711f73c026e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "077f735a-029b-493d-9592-4e1c4f9f074c",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'ae-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'ae-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'ae-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c33ca-9f97-4a30-9a35-86f29d08ec10",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NORMAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73ec76-6d83-4978-9467-9c3605569be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dl.models.pytorch.aedann import AutoEncoder2 as AutoEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae8343-83ba-4250-9dcf-c6d7dedc9b74",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bd27a-493d-4fa5-809b-a87d5779d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/normae_vae0/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=1184,\n",
    "                     mapper=0,\n",
    "                     layer2=339,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=0, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4a816-6a51-4135-b95a-becb5cbbd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=True)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=True)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=True)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=True)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3704776-bf63-4631-aa9b-a177259fa0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116c4b6-6648-464e-82d8-2e535e0e2398",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'normae-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'normae-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'ae-enc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1b6b0-eddd-42f3-aa6c-66f4f43a67b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70a328-5d15-4536-b1ce-4f0c6e27d478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9381fd-3714-41eb-9f8b-9e4a44603341",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group].values, enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7593f-232a-4706-b88f-2fd29d2d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'ae-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'ae-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'ae-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "57839817-23f2-4a0a-ac4b-e1335aea7004",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9db0e3f8-10f7-4111-b6c9-8e31d4786397",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbbfb900-21b6-4cab-bb30-96a42adc1d91",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0eee0d80-31b2-4bc2-8c9f-be46d38de95f",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d074eb2d-0abd-4f59-9cdc-edb84d1c7ea3",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95aafca4-b993-42fb-b11b-762529fcbdf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4aec538b-c1a0-45fb-a307-2477879e1b5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'ae-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'ae-rec', device='cuda')\n",
    "    metrics =log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'ae-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e852e940-c1e7-4555-a19a-92564d9775a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "00a1d9f3-7ccd-4ddd-b406-e34bbc2c7230",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'ae-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'ae-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'ae-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b271430-a7a3-411f-a549-f29a2dee3172",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AEDANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a84a07-bb19-4f9d-89fc-7100cbe8806d",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42669552-e922-4a7d-ab05-0a5854d65c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577, 896)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AutoEncoder2:\n\tsize mismatch for enc.linear1.0.weight: copying a param with shape torch.Size([666, 887]) from checkpoint, the shape in current model is torch.Size([666, 896]).\n\tsize mismatch for dec.linear2.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for dec.linear2.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for _dec_mean.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for _dec_mean.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for _dec_disp.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for _dec_disp.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for _dec_pi.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for _dec_pi.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19708\\2438445212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mbest_ae2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mbest_ae2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{path}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[0mbest_ae2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1668\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AutoEncoder2:\n\tsize mismatch for enc.linear1.0.weight: copying a param with shape torch.Size([666, 887]) from checkpoint, the shape in current model is torch.Size([666, 896]).\n\tsize mismatch for dec.linear2.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for dec.linear2.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for _dec_mean.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for _dec_mean.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for _dec_disp.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for _dec_disp.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for _dec_pi.0.weight: copying a param with shape torch.Size([887, 666]) from checkpoint, the shape in current model is torch.Size([896, 666]).\n\tsize mismatch for _dec_pi.0.bias: copying a param with shape torch.Size([887]) from checkpoint, the shape in current model is torch.Size([896])."
     ]
    }
   ],
   "source": [
    "path = 'data'\n",
    "args.remove_zeros=0\n",
    "data, unique_labels, unique_batches = get_mice(path, args)\n",
    "print(data['inputs']['train'].shape)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('robust', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/DANN_vae0/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=666,\n",
    "                     mapper=0,\n",
    "                     layer2=748,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=0, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806e84c-2a37-40dd-bf05-fe172fb8020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=True, mapping=False)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=True, mapping=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fabbc0-10bf-4156-aab4-027c6ce393f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_data['inputs']['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d672d3-aea5-4ad4-9946-6b194f024e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45957d90-61e7-41bd-9f81-4c604038db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'aedann-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'aedann-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-enc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed11b8-337d-41e6-a500-8906e7187c5c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7004e72-2a56-4c40-b948-4b690721e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group].values, enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dcddab-2564-451e-9b3d-e084573e6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'aedann-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'aedann-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'aedann-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fb40959-81da-49a8-9cf5-eb3b41552247",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa28b037-bd6d-4e18-a5e1-db0121efe0a3",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_mice(path, 'unique_genes.csv', 'CU_DEM-AD')\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('minmax', data, device='cpu')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6caa6f4f-be66-4111-9673-f6515287124f",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in rec_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n",
    "for group in rec_data['inputs']:\n",
    "    rec_data['inputs'][group] = pd.DataFrame(rec_data['inputs'][group])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcf85148-0370-4e61-8803-0e40899ffaea",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50232974-9498-47f9-98c0-d53476f9ce18",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a8c1b70-ef37-43e9-9c2c-e43e0ab3a0fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "508749c4-bea1-4306-ab1f-782937fb868e",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'aedann-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'aedann-rec', device='cuda')\n",
    "    # metrics['aedann-rec']['all']['LDA_score'] = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e0ef210-8f0b-41e1-aadd-6dabdbb8ede3",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'aedann-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'aedann-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'aedann-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b83117-c4a9-4dc3-86bf-5702821386d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AE-invTriplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6b19b-858b-4ab8-9122-fc4f147066ec",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583d621-0f04-444e-9a74-eba2018983e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/inverseTriplet_vae0/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=1543,\n",
    "                     mapper=0,\n",
    "                     layer2=514,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=0, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811406a5-fe78-4aaf-b271-f5d593517f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=True)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=True)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=True)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=True)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=True)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=True)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=True)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=True)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920ce8e-1437-4bb5-a3f7-ca5d77448ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3064f-fc09-4d50-b49c-bb7d3b916741",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'ae-invTriplet-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'ae-invTriplet-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-enc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479013d-2990-4fc5-9413-5e90b8294c72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2328a60-c277-4046-b72b-b15ce5f2f981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97769c15-bbd1-473b-acf4-fde706b8f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group].values, enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5c69d-7440-45b3-82a6-a999cfd6d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'ae-invTriplet-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'ae-invTriplet-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'aedann-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce9d6aa7-66fb-4b02-8107-311ab3344c5d",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6715b446-ebb1-4f93-809b-1736d059a607",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59697991-32a7-4e0f-a080-50daaf2a0c49",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c345ee6-f055-4d4a-a823-c581fa24abf3",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76ede5f3-371a-486e-9cc7-1bf98df22f07",
   "metadata": {},
   "source": [
    "sel = VarianceThreshold(threshold=0.01)\n",
    "sel.fit(data['inputs']['all'])\n",
    "features = np.array([int(x.split('x')[1]) for x in sel.get_feature_names_out()])\n",
    "rec_data['inputs']['all'] = rec_data['inputs']['all'][:, features]\n",
    "rec_data['inputs']['all_pool'] = rec_data['inputs']['all_pool'][:, features]\n",
    "rec_data['inputs']['train'] = rec_data['inputs']['train'][:, features]\n",
    "rec_data['inputs']['train_pool'] = rec_data['inputs']['train_pool'][:, features]\n",
    "rec_data['inputs']['valid'] = rec_data['inputs']['valid'][:, features]\n",
    "rec_data['inputs']['valid_pool'] = rec_data['inputs']['valid_pool'][:, features]\n",
    "rec_data['inputs']['test'] = rec_data['inputs']['test'][:, features]\n",
    "rec_data['inputs']['test_pool'] = rec_data['inputs']['test_pool'][:, features]\n",
    "\n",
    "# rec_data['inputs']['all'].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e048bb4-a6c3-4e64-adf5-6ad9ce7a43a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b1a93f1-aef7-4aa9-bfd8-bdc8deaa64cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d229220e-3b1c-4329-a48c-fe0766b67141",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'aedann-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'aedann-rec', device='cuda')\n",
    "    metrics['aedann-rec']['all']['LDA_score'] = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b361b336-2b67-42f0-a875-1f2c86080ed2",
   "metadata": {},
   "source": [
    "sel = VarianceThreshold(threshold=0.95)\n",
    "sel.fit(enc_data['inputs']['all'])\n",
    "features = np.array([int(x.split('x')[1]) for x in sel.get_feature_names_out()])\n",
    "enc_data['inputs']['all'] = enc_data['inputs']['all'][:, features]\n",
    "enc_data['inputs']['all_pool'] = enc_data['inputs']['all_pool'][:, features]\n",
    "enc_data['inputs']['train'] = enc_data['inputs']['train'][:, features]\n",
    "enc_data['inputs']['train_pool'] = enc_data['inputs']['train_pool'][:, features]\n",
    "enc_data['inputs']['valid'] = enc_data['inputs']['valid'][:, features]\n",
    "enc_data['inputs']['valid_pool'] = enc_data['inputs']['valid_pool'][:, features]\n",
    "enc_data['inputs']['test'] = enc_data['inputs']['test'][:, features]\n",
    "enc_data['inputs']['test_pool'] = enc_data['inputs']['test_pool'][:, features]\n",
    "\n",
    "enc_data['inputs']['all'].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bd02d9b-4857-4935-814b-e415da4657ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2203dda1-a47f-449a-982b-b18a83c3b206",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a25f91a2-572f-4b36-afcc-31bef517c4f9",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "09eb4d1d-3588-4821-bfa1-4a021571bf62",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'aedann-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'aedann-rec', device='cuda')\n",
    "    # metrics = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bde94d6-2882-4c35-9ed8-05a448b70061",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'aedann-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'aedann-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'aedann-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8f298-4bdc-44d4-bd14-fd6dd57ba565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04693986-a26c-4048-a325-c3dd3dc41ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AE-revTriplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85fcc4-79f6-4eca-bc91-f19d4f260f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cf88620-0d3b-4d6a-97d2-8b639e3bbd0f",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d034e-0c52-4895-90cb-fc14715bc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/revTriplet_vae0/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=1365,\n",
    "                     mapper=0,\n",
    "                     layer2=493,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=0, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35539dc1-3045-41bd-914c-3f02e6a8d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=True)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=True)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=True)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=True)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a71fd-c76c-46b5-b254-ed07ec9906f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f42dce-1b4f-4a65-aeff-2edb2d37b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'ae-revTriplet-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'ae-revTriplet-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-enc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad433cc4-cc32-4704-87bb-719f330c1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group], enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba081ba-1a51-455d-b83e-3f1fd9a1212d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b7db8-7f41-4a82-8059-d13d3882aa75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b79ed-0dc6-4662-8a27-79986b29c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'ae-revTriplet-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'ae-revTriplet-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'aedann-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b948320e-2046-49bd-9c5a-d6aeeaca6439",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c95969c2-90f4-42f7-9efe-e30a0170e960",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19857527-cadc-4b7c-82c7-3e85dba2fcd4",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cdca2e7-2e31-4cd9-80bb-b68d89dd97ff",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "caf9973c-ed47-4350-a06e-86ec424d023f",
   "metadata": {},
   "source": [
    "sel = VarianceThreshold(threshold=0.01)\n",
    "sel.fit(data['inputs']['all'])\n",
    "features = np.array([int(x.split('x')[1]) for x in sel.get_feature_names_out()])\n",
    "rec_data['inputs']['all'] = rec_data['inputs']['all'][:, features]\n",
    "rec_data['inputs']['all_pool'] = rec_data['inputs']['all_pool'][:, features]\n",
    "rec_data['inputs']['train'] = rec_data['inputs']['train'][:, features]\n",
    "rec_data['inputs']['train_pool'] = rec_data['inputs']['train_pool'][:, features]\n",
    "rec_data['inputs']['valid'] = rec_data['inputs']['valid'][:, features]\n",
    "rec_data['inputs']['valid_pool'] = rec_data['inputs']['valid_pool'][:, features]\n",
    "rec_data['inputs']['test'] = rec_data['inputs']['test'][:, features]\n",
    "rec_data['inputs']['test_pool'] = rec_data['inputs']['test_pool'][:, features]\n",
    "\n",
    "# rec_data['inputs']['all'].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0379cb5-bafd-4ed4-bdb2-26b5e773a020",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed434cf9-3440-4082-9b32-399de69034a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3f9ac090-f32c-442e-a0c1-abbdc8c5286e",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'aedann-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'aedann-rec', device='cuda')\n",
    "    metrics['aedann-rec']['all']['LDA_score'] = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28656414-1171-4af8-9c65-c0ad4246b5b7",
   "metadata": {},
   "source": [
    "sel = VarianceThreshold(threshold=0.95)\n",
    "sel.fit(enc_data['inputs']['all'])\n",
    "features = np.array([int(x.split('x')[1]) for x in sel.get_feature_names_out()])\n",
    "enc_data['inputs']['all'] = enc_data['inputs']['all'][:, features]\n",
    "enc_data['inputs']['all_pool'] = enc_data['inputs']['all_pool'][:, features]\n",
    "enc_data['inputs']['train'] = enc_data['inputs']['train'][:, features]\n",
    "enc_data['inputs']['train_pool'] = enc_data['inputs']['train_pool'][:, features]\n",
    "enc_data['inputs']['valid'] = enc_data['inputs']['valid'][:, features]\n",
    "enc_data['inputs']['valid_pool'] = enc_data['inputs']['valid_pool'][:, features]\n",
    "enc_data['inputs']['test'] = enc_data['inputs']['test'][:, features]\n",
    "enc_data['inputs']['test_pool'] = enc_data['inputs']['test_pool'][:, features]\n",
    "\n",
    "enc_data['inputs']['all'].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ef52468-3ccc-4292-8c66-8e1734e72de3",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "999e191c-1def-4d50-80d8-a9d6cefd24a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95575397-a069-4b64-8712-d0494ff3d5b3",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e0b0ccd4-a462-4c96-8f50-94300a0cff53",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'aedann-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'aedann-rec', device='cuda')\n",
    "    # metrics = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9f4bbc2-2233-464a-995d-70404c96328a",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'aedann-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'aedann-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'aedann-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c2dc4-8e43-424b-93ce-2c7529000995",
   "metadata": {},
   "source": [
    "# NORMVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eefce3-9600-432b-951b-2a8a4a23f9bd",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527794f5-32c9-43c1-8d9b-0113fa8ce8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/normae_vae1/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=1181,\n",
    "                     mapper=0,\n",
    "                     layer2=433,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=1, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8b553-3356-4f05-8bdf-fa3b6562e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=False)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c0f22-e169-49a8-84a6-e639c0824d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbffd3f-0d9d-425b-959e-4923b4a7a5d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'normvae-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'normvae-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'vae-enc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57fefe-d21c-4614-9ff1-4ece9ad6742b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308930e-42a5-4d26-8531-9495b537a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group], enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae896f-5902-4fa6-9f8b-331511a08e99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936aca8-6024-474d-942e-9cf44436997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'vae-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'vae-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'vae-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b454f47-ae24-4fb5-85d3-739d0e00d640",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f84805ab-997a-4bff-9cca-553c5d78e1ea",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f67871b-85f5-45c8-ac84-f78fa5dba3f3",
   "metadata": {},
   "source": [
    "data['inputs'][group].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16fee744-3c0c-473b-b1fa-64a710bbecab",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11ce81b7-97bc-413f-8a25-772181c09af1",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c065f51-f98c-4bd6-b9cf-9c437ec72c85",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ada523da-b7af-4f70-a6c1-ed711e2625f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8a0deb3a-c936-4aff-8718-b87fad938408",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'vae-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'vae-rec', device='cuda')\n",
    "    # metrics = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'vae-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "568744a6-f2e8-4597-adcf-750034a238f5",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'vae-rec', n_meta)\n",
    "    train_linsvc(enc_data, 'vae-rec', n_meta)\n",
    "    # train_logreg(enc_data, 'vae-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcab8ce-0ef3-4fcc-bfe8-50c0d43f5002",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3985bdb-e34a-49c9-aad8-1f86c4ee6973",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6f71c-f138-471a-af05-a0402ec0b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/no_vae1/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=1629,\n",
    "                     mapper=0,\n",
    "                     layer2=136,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=1, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140bd36-b26f-4e97-beec-933e1c2f3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=False)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e5f55-9299-47dd-be21-ff32c158d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08d769-2233-4f5a-8053-7b3e0ba01e48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'vae-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'vae-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'vae-enc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c8ceb-6ad1-4149-a518-02bb17e53046",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364e228-fa9b-430f-a771-fe5483c4ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group], enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b95e2-bb4d-4e99-90d3-ed257d2daf30",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2c023-1844-4f35-8cc5-897669738703",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'vae-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'vae-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'vae-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "99a68e3d-58a2-4cd0-aa78-b91f6aa73ea1",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a59c874-06c5-44f6-acc1-e314a185b2ba",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfd077c0-85c0-49c6-a02a-a0a9ad4fa453",
   "metadata": {},
   "source": [
    "data['inputs'][group].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2953efe5-e37b-4aad-9673-f70b0ff3c8d8",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4a489de-d312-4f03-a77c-b1e329fc81ca",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "144f83f8-dfee-487b-9d7a-c1892ed9b2dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05720704-511a-42b1-9ae9-17af81af5b9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3b015179-ca1d-4b41-a0a3-4f0cf531a60b",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'vae-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'vae-rec', device='cuda')\n",
    "    # metrics = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'vae-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0144cd17-98ab-4422-a96b-ca2706b408ab",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'vae-rec', n_meta)\n",
    "    train_linsvc(enc_data, 'vae-rec', n_meta)\n",
    "    # train_logreg(enc_data, 'vae-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c7cbf-3190-4bb4-b388-ce6678d0201f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VAEDANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035fa0d-6e8e-43d5-97f0-864d03e6c43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac57b724-fa84-479f-a7df-dd2b331cefbf",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e889fb-b11a-4771-bb63-24689bbbc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice('data/Alzheimer', csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/DANN_vae1/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=941,\n",
    "                     mapper=0,\n",
    "                     layer2=265,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=1, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d236298-60a0-44af-a97c-8443c45e4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=False)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596daa9-4624-4e3a-9b58-4dae63870cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf1939-3923-4b26-8e6b-a49dd9a5b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'vaedann-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'vaedann-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-enc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96d6d0-3300-4afc-80d0-3a8fbc4dd98f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53b94c-3d29-4546-bfb5-0c1ae425bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group].values, enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca546f-7726-4324-b071-704340b51e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'vaedann-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'vaedann-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'aedann-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "54f12fce-d262-4804-bb19-8a3cda03f378",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31cfee9f-64f2-40f2-bf10-1a2b9db9ff34",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_mice(path, 'unique_genes.csv', 'CU_DEM-AD')\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('minmax', data, device='cpu')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aabd16cd-5fd7-40b2-94f3-5d6ac63754b4",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in rec_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n",
    "for group in rec_data['inputs']:\n",
    "    rec_data['inputs'][group] = pd.DataFrame(rec_data['inputs'][group])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d18ce9d-84d0-4019-8d17-9035437b9972",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'].iloc[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7cd1d17-d4a4-4303-a748-dff3bd744944",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6d2424f-2522-48ff-96c6-dc8b4e9e5a08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "93abbec9-aa21-4422-93e7-16fd05346c4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'vaedann-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'vaedann-rec', device='cuda')\n",
    "    # metrics['aedann-rec']['all']['LDA_score'] = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c26b095c-93b8-4e0a-8c35-e7c90121ab4b",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'vaedann-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'vaedann-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'aedann-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8316a0-33ec-4a0c-8c38-10c09bfd869a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VAE-invTriplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9480c263-e000-46c4-a954-9bc1888588d1",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc8d49-daf0-441d-abf2-52c29f8f4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/inverseTriplet_vae1/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=1930,\n",
    "                     mapper=0,\n",
    "                     layer2=372,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=1, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14956f31-c112-41d4-a6e4-775c7e56ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=False)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e87e7-cb62-4e83-a40a-4aa6b4788b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef90e07-496b-4e5a-bcc9-d3b9da4ea068",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'vae-invTriplet-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'vae-invTriplet-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-enc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64067699-4ff3-48b9-ade6-df7d8460a1d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78c808-4a5e-4ba1-8d8c-c5dafb714eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d09d6-a48b-4c9b-95ef-bcfee46170d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group].values, enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c6948-16de-4301-8b4c-df5e57d1d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'vae-invTriplet-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'vae-invTriplet-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'aedann-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "507effe5-f2ae-40b1-a8c9-768287f19669",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ea11759-1e39-4d2e-a91a-0cfa0983b01d",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dff22d15-de70-435e-8d80-b799e4122df9",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c4c42cb-68e5-4721-b25d-891c31698f05",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a4361a3-02b9-4628-9336-63ba9cde0348",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28751a64-acee-48ce-be71-14fb33a974ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4326a742-70a3-43bd-b3e5-70670886a05c",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'ae-revTriplet-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'ae-revTriplet-rec', device='cuda')\n",
    "    # metrics['ae-revTriplet-rec']['all']['LDA_score'] = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41bde855-5c64-45ab-91fe-176703a52130",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23418371-6b00-4184-8f6b-f9c57658c399",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d42d1688-4f1d-44d0-ad59-b3d3e43bd1e8",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0c973b1b-0eda-4f68-98f5-cbbb63c4a353",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'ae-revTriplet-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'ae-revTriplet-rec', device='cuda')\n",
    "    # metrics = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ccab136-a6bf-4103-82af-0ee31f482fe9",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'ae-revTriplet-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'ae-revTriplet-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'aedann-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14a851-361f-4b9f-a9bd-5b55fd535111",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VAE-revTriplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca621422-a516-4996-b047-7e24a3bac528",
   "metadata": {},
   "source": [
    "## Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e20180-8bd6-4029-b842-d487cbfd930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data, unique_labels, unique_batches = get_mice(path, csv_name, bad_batches, remove_zeros=0)\n",
    "# unique_labels = get_unique_labels(data['labels']['all'])\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "data, _ = scale_data('standard', data, device='cpu')\n",
    "\n",
    "if not best_correction:\n",
    "    # Best score run Brain-1446\n",
    "    path='logs/best_models_server/ae_then_classifier_holdout/unique_genes/revTriplet_vae1/model_1.pth'\n",
    "else:\n",
    "    # Run Brain-\n",
    "    path='logs/ae_classifier_holdout/.../model_3.pth'\n",
    "    \n",
    "# best score is autoencoder0, best correction autoencoder3\n",
    "if not best_correction:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                     n_batches=22,\n",
    "                     nb_classes=2,\n",
    "                     layer1=561,\n",
    "                     mapper=0,\n",
    "                     layer2=893,\n",
    "                     dropout=0,\n",
    "                           n_meta=0,\n",
    "                           n_emb=2,\n",
    "                           n_layers=2,\n",
    "                     variational=1, conditional=False, zinb=0,\n",
    "                     add_noise=0, tied_weights=0, \n",
    "                     use_gnn=0, device='cpu').to('cpu')\n",
    "else:\n",
    "    best_ae2 = AutoEncoder(data['inputs']['all'].shape[1],\n",
    "                 n_batches=21,\n",
    "                 nb_classes=2,\n",
    "                 layer1=595,\n",
    "                 mapper=0,\n",
    "                 layer2=266,\n",
    "                 dropout=0,\n",
    "                 variational=0, conditional=False, zinb=0,\n",
    "                 add_noise=0, tied_weights=0, n_meta=2,\n",
    "                 use_gnn=0, device='cpu').to('cpu')\n",
    "\n",
    "best_ae2.mapper.to('cpu')\n",
    "best_ae2.dec.to('cpu')\n",
    "\n",
    "best_ae2.load_state_dict(torch.load(f'{path}', map_location='cpu'))\n",
    "best_ae2.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e18044-336b-4599-a066-1cad609dca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']]).detach().cpu()\n",
    "enc_data = data.copy()\n",
    "enc_data['inputs']['all'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all'].values), batches, sampling=False)\n",
    "enc_data['inputs']['all_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train'].values), batches, sampling=False)\n",
    "enc_data['inputs']['train_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid'].values), batches, sampling=False)\n",
    "enc_data['inputs']['valid_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test'].values), batches, sampling=False)\n",
    "enc_data['inputs']['test_pool'], _, _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool'].values), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    enc_data['inputs'][group] = pd.DataFrame(enc_data['inputs'][group].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de4440-a8d0-4080-8b28-289128f38fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                enc_data['inputs']['all'].values[np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c81b4b-8ac3-44a6-af4a-53904f1421d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(enc_data['inputs'], enc_data['batches'], metrics, 'vae-revTriplet-enc')\n",
    "    metrics = log_metrics(enc_data, unique_labels, enc_data['batches'], metrics, 'vae-revTriplet-enc', device='cuda')\n",
    "    # metrics = log_LDA(LDA, enc_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-enc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f887915-f0cf-4e4e-8818-a213091f8ec5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, \n",
    "         # 'age': unique_ages\n",
    "        }, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f2591-3f2e-4029-b058-d4c2c0b78b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e93336-00b3-4ab0-8686-1e977e7d1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in enc_data['inputs']:\n",
    "    if n_meta == 2:\n",
    "        enc_data['inputs'][group] = pd.DataFrame(np.concatenate((enc_data['inputs'][group].values, enc_data['meta'][group]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d69f32-d41a-4e24-b0ed-d022fe52b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_models:\n",
    "    train_rfc(enc_data, 'vae-revTriplet-enc', n_meta)\n",
    "    train_linsvc(enc_data, 'vae-revTriplet-enc', n_meta)\n",
    "    # train_logreg(enc_data, 'aedann-enc', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1de68b21-b0f9-4c90-95a4-f8c9eb06f7db",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be3dd2ee-29e0-4be0-9f99-c8656135fb52",
   "metadata": {},
   "source": [
    "path = 'data'\n",
    "data, unique_labels = get_data(path, csv_name, bad_batches)\n",
    "unique_batches = np.unique(data['batches']['all'])\n",
    "unique_cats = np.unique(data['cats']['all'])\n",
    "unique_ages = np.array(['50s', '60s', '70s', '80+'])\n",
    "unique_genders = np.unique(data['meta']['all'].iloc[:, 1])\n",
    "n_cats = len(unique_labels)\n",
    "n_batches = len(unique_batches)\n",
    "n_ages = len(unique_ages)\n",
    "n_genders = len(unique_genders)\n",
    "data['age'] = {}\n",
    "data['gender'] = {}\n",
    "meta_age = []\n",
    "for age in data['meta']['all'].iloc[:, 0]:\n",
    "    if age < 50:\n",
    "        meta_age += ['pool']\n",
    "    elif age < 60:\n",
    "        meta_age += ['50s']\n",
    "    elif age < 70:\n",
    "        meta_age += ['60s']\n",
    "    elif age < 80:\n",
    "        meta_age += ['70s']\n",
    "    else:\n",
    "        meta_age += ['80+']\n",
    "data['age']['all'] = np.array(meta_age)\n",
    "data['gender']['all'] = data['meta']['all'].iloc[:, 1]\n",
    "\n",
    "scaler = Pipeline([('standard', RobustScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = Pipeline([('standard', StandardScaler()), ('minmax', MinMaxScaler())])\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "data['inputs']['all'] = scaler.fit_transform(data['inputs']['all'])\n",
    "data['inputs']['all_pool'] = scaler.transform(data['inputs']['all_pool'])\n",
    "data['inputs']['train'] = scaler.transform(data['inputs']['train'])\n",
    "data['inputs']['train_pool'] = scaler.transform(data['inputs']['train_pool'])\n",
    "data['inputs']['valid'] = scaler.transform(data['inputs']['valid'])\n",
    "data['inputs']['valid_pool'] = scaler.transform(data['inputs']['valid_pool'])\n",
    "data['inputs']['test'] = scaler.transform(data['inputs']['test'])\n",
    "data['inputs']['test_pool'] = scaler.transform(data['inputs']['test_pool'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "729024c3-9328-48cf-aea8-d453bafc902c",
   "metadata": {},
   "source": [
    "batches = torch.Tensor([np.argwhere(unique_batches == x)[0][0] for x in data['batches']['all']])\n",
    "rec_data = data.copy()\n",
    "_, rec_data['inputs']['all'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all']), batches, sampling=False)\n",
    "_, rec_data['inputs']['all_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['all_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train']), batches, sampling=False)\n",
    "_, rec_data['inputs']['train_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['train_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid']), batches, sampling=False)\n",
    "_, rec_data['inputs']['valid_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['valid_pool']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test']), batches, sampling=False)\n",
    "_, rec_data['inputs']['test_pool'], _, _ = best_ae2.cpu()(torch.Tensor(data['inputs']['test_pool']), batches, sampling=False)\n",
    "\n",
    "for group in enc_data['inputs']:\n",
    "    rec_data['inputs'][group] = rec_data['inputs'][group]['mean'][-1].detach().cpu().numpy()\n",
    "    if n_meta == 2:\n",
    "        rec_data['inputs'][group] = np.concatenate((rec_data['inputs'][group], rec_data['meta'][group]), 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1ecf1b1-4173-433b-8ef4-156a21fa56e1",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Mann      pval min    n pvals < 0.05\")\n",
    "table = pd.DataFrame(columns=['pval', 'n'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.mannwhitneyu(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n",
    "\n",
    "print('ttests')\n",
    "table = pd.DataFrame(columns=['pval'])\n",
    "i = 0\n",
    "for i, label in enumerate(unique_labels[:-1]):\n",
    "    for label2 in unique_labels[i+1:]:\n",
    "        if label != label2 and label != 'pool' and label2 != 'pool':\n",
    "            pvals = stats.ttest_ind(\n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label).squeeze()], \n",
    "                rec_data['inputs']['all'][np.argwhere(data['labels']['all'] == label2).squeeze()]\n",
    "            )\n",
    "            tmp = multipletests(pvals[1], 0.05, 'fdr_bh')[1]\n",
    "            table.loc[f'{label}_{label2}', 'pval'] = tmp.min()\n",
    "            table.loc[f'{label}_{label2}', 'n'] = len([x for x in tmp if x < 0.05])\n",
    "            i += 1\n",
    "print(tabulate(table))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15aaa320-752f-42c1-9d09-e9b9563d6044",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_recs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8c3806c-2dc4-4cfd-91b0-017ac45a6203",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "be928da4-8c9b-4eb8-918c-4600e74f9ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'ae-revTriplet-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'ae-revTriplet-rec', device='cuda')\n",
    "    # metrics['ae-revTriplet-rec']['all']['LDA_score'] = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55a7765f-fe6c-4174-b392-5dcb2050d0cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, enc_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb34cbda-3fb7-48de-9423-0fb60afba2ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "log_ORD({'model': PCA(n_components=2), 'name': f'PCA_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n",
    "log_ORD({'model': UMAP(n_components=2), 'name': f'UMAP_encs_labels'}, rec_data, \n",
    "        {'batches': unique_batches, 'labels': unique_labels, 'age': unique_ages}, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b54695f-c230-42a2-a0bb-cb2bf9c5d79d",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a8f837d7-d606-4ad3-8469-38ec0818205c",
   "metadata": {
    "tags": []
   },
   "source": [
    "if log_stuff:\n",
    "    metrics = log_pool_metrics(rec_data['inputs'], rec_data['batches'], metrics, 'ae-revTriplet-rec')\n",
    "    metrics = log_metrics(rec_data, unique_labels, rec_data['batches'], metrics, 'ae-revTriplet-rec', device='cuda')\n",
    "    # metrics = log_LDA(LDA, rec_data, {'batches': unique_batches, 'labels': unique_labels}, 0, metrics, 'aedann-rec')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d89488d-ec05-4c62-a915-c261bc41e9f6",
   "metadata": {},
   "source": [
    "if train_models:\n",
    "    train_rfc(rec_data, 'ae-revTriplet-rec', n_meta)\n",
    "    train_linsvc(rec_data, 'ae-revTriplet-rec', n_meta)\n",
    "    # train_logreg(rec_data, 'aedann-rec', n_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad4a86-71ca-4357-89d4-c58b841b3871",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be0b48-4437-413c-9674-3d7b1a434323",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_stuff:\n",
    "    # table = pd.DataFrame(columns=list(metrics['raw']['all'].keys()) + ['delta', 'delta_pool'], index=list(metrics.keys()))\n",
    "    cols = ['qc_aPCC', '[qc_dist/tot_eucl]', 'lisi', 'silhouette', 'kbet', 'shannon', 'adjusted_rand_score', 'adjusted_mutual_info_score']\n",
    "    cols_pool = ['pool lisi', 'pool silhouette', 'pool kbet', 'pool shannon', 'pool adjusted_rand_score', 'pool adjusted_mutual_info_score',]\n",
    "    table = pd.DataFrame(columns=cols + cols_pool, index=list(metrics.keys()))\n",
    "    # table = table.drop(['qc_dist', 'b_euclidean', 'euclidean', \"[b_euclidean/tot_eucl]\"], 1)\n",
    "    for col in cols:\n",
    "        for row in list(table.index):\n",
    "            # if 'delta' in col:\n",
    "            #     table[col][row] = metrics[row][col]\n",
    "            # else:\n",
    "            try:\n",
    "                if isinstance(metrics[row]['all'][col], dict):\n",
    "                    table[col][row] = metrics[row]['all'][col][\"domains\"]\n",
    "                    try:\n",
    "                        table[f'pool {col}'][row] = metrics[row]['all_pool'][col][\"domains\"]\n",
    "                    except:\n",
    "                        pass\n",
    "                    # table[col][row][\"domains\"] = metrics[row]['all'][col][\"domains\"]\n",
    "                    # table[col][row][\"labels\"] = metrics[row]['all'][col][\"labels\"]\n",
    "                else:\n",
    "                    try:\n",
    "                        table[col][row] = metrics[row]['valid'][col]\n",
    "                    except:\n",
    "                        table[col][row] = metrics[row]['all'][col]\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    table[col][row] = metrics[row]['all_pool'][col]\n",
    "                except:\n",
    "                    table[col][row] = metrics[row]['valid'][col]\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a0262-03ef-47da-8895-6d7b1b86f126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
